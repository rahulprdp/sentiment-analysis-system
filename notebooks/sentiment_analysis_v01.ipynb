{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PAI_V1.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Sentiment Analysis Prototype\n","- Jerin Shaji - AM.EN.U4CSE19325\n","- Rahul P P - AM.EN.U4CSE19344\n","- Saran S Krishna - AM.EN.U4CSE19344\n","- Sidharth Sreehari - AM.EN.U4CSE19351\n"],"metadata":{"id":"okteek9cP0tE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"i-QYOOfT2Npm"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import math"]},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import twitter_samples\n","import numpy as np"],"metadata":{"id":"FS3yuQl5fXaw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('twitter_samples')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sLMUliMLftv_","executionInfo":{"status":"ok","timestamp":1650520098893,"user_tz":-330,"elapsed":1242,"user":{"displayName":"Rahul P P","userId":"05848758069387796897"}},"outputId":"3dacdb94-f5b0-4480-8344-e38a8fc8ded9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# select the set of positive and negative tweets\n","all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n","all_negative_tweets = twitter_samples.strings('negative_tweets.json')"],"metadata":{"id":"CI0ZGI4Df2w0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Number of positive tweets: ', len(all_positive_tweets))\n","print('Number of negative tweets: ', len(all_negative_tweets))\n","\n","print('\\nThe type of all_positive_tweets is: ', type(all_positive_tweets))\n","print('The type of a tweet entry is: ', type(all_negative_tweets[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sy8qOMD0f_B3","executionInfo":{"status":"ok","timestamp":1650520099506,"user_tz":-330,"elapsed":5,"user":{"displayName":"Rahul P P","userId":"05848758069387796897"}},"outputId":"aeb850ff-e3be-4edf-a134-eb553d01aeea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of positive tweets:  5000\n","Number of negative tweets:  5000\n","\n","The type of all_positive_tweets is:  <class 'list'>\n","The type of a tweet entry is:  <class 'str'>\n"]}]},{"cell_type":"code","source":["print(\"Positive Tweet Example:\")\n","print(all_positive_tweets[0])\n","\n","print(\"\\nNegative Tweet Example:\")\n","print(all_negative_tweets[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"laOxI8R7gDgV","executionInfo":{"status":"ok","timestamp":1650520099907,"user_tz":-330,"elapsed":404,"user":{"displayName":"Rahul P P","userId":"05848758069387796897"}},"outputId":"015f586a-2200-47c1-a902-5bc1eb25697d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Positive Tweet Example:\n","#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n","\n","Negative Tweet Example:\n","hopeless for tmr :(\n"]}]},{"cell_type":"markdown","source":["### Preprocess Tweets"],"metadata":{"id":"Vcne60mkgGzX"}},{"cell_type":"code","source":["import re                                  # library for regular expression operations\n","import string                              # for string operations\n","\n","from nltk.corpus import stopwords          # module for stop words that come with NLTK\n","from nltk.stem import PorterStemmer        # module for stemming\n","from nltk.tokenize import TweetTokenizer   # module for tokenizing strings"],"metadata":{"id":"v0ksYu7JgI3A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Remove hyperlinks, Twitter marks and styles\n","We do not want to use every word in a tweet because many tweets have hashtags, retweet marks, and hyperlinks. We use regular expressions to remove them from a tweet."],"metadata":{"id":"WnWPIHG3gOzA"}},{"cell_type":"code","source":["def remove_hyperlinks_marks_styles(tweet):\n","    \n","    # remove old style retweet text \"RT\"\n","    new_tweet = re.sub(r'^RT[\\s]+', '', tweet)\n","\n","    # remove hyperlinks\n","    new_tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', new_tweet)\n","\n","    # remove hashtags\n","    # only removing the hash # sign from the word\n","    new_tweet = re.sub(r'#', '', new_tweet)\n","    \n","    return new_tweet"],"metadata":{"id":"GbElC8eDgLgM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Tokenize the string\n","Splitting a string into individual words."],"metadata":{"id":"Th1L3mcChDWX"}},{"cell_type":"code","source":["# instantiate tokenizer class\n","tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n","                               reduce_len=True)\n","\n","def tokenize_tweet(tweet):\n","    \n","    tweet_tokens = tokenizer.tokenize(tweet)\n","    \n","    return tweet_tokens"],"metadata":{"id":"Olw6WE7-ggho"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Remove stop works and punctuations"],"metadata":{"id":"CZEDHvl2hb-k"}},{"cell_type":"code","source":["nltk.download('stopwords')\n","\n","#Import the english stop words list from NLTK\n","stopwords_english = stopwords.words('english')\n","\n","punctuations = string.punctuation\n","\n","def remove_stopwords_punctuations(tweet_tokens):\n","    \n","    tweets_clean = []\n","    \n","    for word in tweet_tokens:\n","        if (word not in stopwords_english and word not in punctuations):\n","            tweets_clean.append(word)\n","            \n","    return tweets_clean"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sR4VozuihYln","executionInfo":{"status":"ok","timestamp":1650520099910,"user_tz":-330,"elapsed":19,"user":{"displayName":"Rahul P P","userId":"05848758069387796897"}},"outputId":"b0f43907-25c7-429a-fe1c-7d5333b48ffb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"markdown","source":["### Stemming"],"metadata":{"id":"wr2ojBfwhjn2"}},{"cell_type":"code","source":["stemmer = PorterStemmer()\n","\n","def get_stem(tweets_clean):\n","    \n","    tweets_stem = []\n","    \n","    for word in tweets_clean:\n","        stem_word = stemmer.stem(word)\n","        tweets_stem.append(stem_word)\n","        \n","    return tweets_stem"],"metadata":{"id":"_MWb1N8Hhkye"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tweet_example = all_positive_tweets[2277]\n","print(tweet_example)\n","\n","processed_tweet = remove_hyperlinks_marks_styles(tweet_example)\n","print(\"\\nRemoved hyperlinks, Twitter marks and styles:\")\n","print(processed_tweet)\n","\n","tweet_tokens = tokenize_tweet(processed_tweet)\n","print(\"\\nTokenize the string:\")\n","print(tweet_tokens)\n","\n","tweets_clean = remove_stopwords_punctuations(tweet_tokens)\n","print(\"\\nRemove stop words and punctuations:\")\n","print(tweets_clean)\n","\n","tweets_stem = get_stem(tweets_clean)\n","print(\"\\nGet stem of each word:\")\n","print(tweets_stem)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SXpq9DBohxKw","executionInfo":{"status":"ok","timestamp":1650520099911,"user_tz":-330,"elapsed":18,"user":{"displayName":"Rahul P P","userId":"05848758069387796897"}},"outputId":"b6c60f94-65ae-48cb-c868-197b1411c747"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n","\n","Removed hyperlinks, Twitter marks and styles:\n","My beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… \n","\n","Tokenize the string:\n","['my', 'beautiful', 'sunflowers', 'on', 'a', 'sunny', 'friday', 'morning', 'off', ':)', 'sunflowers', 'favourites', 'happy', 'friday', 'off', '…']\n","\n","Remove stop words and punctuations:\n","['beautiful', 'sunflowers', 'sunny', 'friday', 'morning', ':)', 'sunflowers', 'favourites', 'happy', 'friday', '…']\n","\n","Get stem of each word:\n","['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '…']\n"]}]},{"cell_type":"markdown","source":["### Combine all preprocess techniques"],"metadata":{"id":"GjcB6VGKioI-"}},{"cell_type":"code","source":["def process_tweet(tweet):\n","    \n","    processed_tweet = remove_hyperlinks_marks_styles(tweet)\n","    tweet_tokens = tokenize_tweet(processed_tweet)\n","    tweets_clean = remove_stopwords_punctuations(tweet_tokens)\n","    tweets_stem = get_stem(tweets_clean)\n","    \n","    return tweets_stem"],"metadata":{"id":"0qGjdJAyiQEj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tweet_example = all_negative_tweets[1000]\n","print(tweet_example)\n","\n","processed_tweet = process_tweet(tweet_example)\n","print(processed_tweet)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mjojmq_jiw-R","executionInfo":{"status":"ok","timestamp":1650520099912,"user_tz":-330,"elapsed":16,"user":{"displayName":"Rahul P P","userId":"05848758069387796897"}},"outputId":"bf83d68a-b667-4428-a17e-36b0aaa54e78"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["@seanactual You mean you're not offering? :(\n","['mean', 'offer', ':(']\n"]}]},{"cell_type":"markdown","source":["### Split data into two pieces, one for training and one for testing"],"metadata":{"id":"S2B2LJOMjWsm"}},{"cell_type":"code","source":["test_pos = all_positive_tweets[4000:]\n","train_pos = all_positive_tweets[:4000]\n","test_neg = all_negative_tweets[4000:]\n","train_neg = all_negative_tweets[:4000]\n","\n","train_x = train_pos + train_neg\n","test_x = test_pos + test_neg\n","\n","train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n","test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"],"metadata":{"id":"QKp7Jur1jX8J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create frequency dictionary"],"metadata":{"id":"1OY61q05jgZ4"}},{"cell_type":"code","source":["def create_frequency(tweets, ys):\n","    \n","    freq_d = {}\n","\n","    for tweet,y in zip(tweets,ys):\n","      for word in process_tweet(tweet):\n","        pair = (word,y)\n","\n","        if pair in freq_d:\n","          freq_d[pair] +=1\n","        else:\n","          freq_d[pair] = freq_d.get(pair,1)\n","    \n","    return freq_d"],"metadata":{"id":"56YFMQ1PjiNV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# testing function\n","\n","tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n","ys = [1, 0, 0, 0, 0]\n","\n","freq_d = create_frequency(tweets, ys)\n","print(freq_d)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XyTt0SuIkgIW","executionInfo":{"status":"ok","timestamp":1650520099913,"user_tz":-330,"elapsed":13,"user":{"displayName":"Rahul P P","userId":"05848758069387796897"}},"outputId":"7358f696-7573-4535-e066-6320efc9e608"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}\n"]}]},{"cell_type":"markdown","source":["### Train model using Naive Bayes"],"metadata":{"id":"HbKSq4MflYMp"}},{"cell_type":"code","source":["# build the freqs dictionary\n","\n","freqs = create_frequency(train_x, train_y)"],"metadata":{"id":"e7ygy41RlXrB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_naive_bayes(freqs, train_x, train_y):\n","    '''\n","    Input:\n","        freqs: dictionary from (word, label) to how often the word appears\n","        train_x: a list of tweets\n","        train_y: a list of labels correponding to the tweets (0,1)\n","    Output:\n","        logprior: the log prior. (equation 3 above)\n","        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n","    '''\n","    \n","    loglikelihood = {}\n","    logprior = 0\n","    \n","    # calculate the number of unique words in vocab\n","    unique_words = set([pair[0] for pair in freqs.keys()])\n","    V = len(unique_words)\n","    \n","    # calculate N_pos and N_neg\n","    N_pos = N_neg = 0\n","    for pair in freqs.keys():\n","        \n","        # TODO: get N_pos and N_get\n","        if pair[1]>0:\n","          N_pos += freqs[(pair)]\n","        else:\n","          N_neg +=freqs[(pair)]\n","            \n","    # TODO: calculate the number of documents (tweets)\n","    D = train_y.shape[0]\n","    \n","    # TODO: calculate D_pos, the number of positive documents (tweets)\n","    D_pos = sum(train_y)\n","    \n","    # TODO: calculate D_neg, the number of negative documents (tweets)\n","    D_neg = D - sum(train_y)\n","    \n","    # TODO: calculate logprior\n","    logprior = np.log(D_pos) - np.log(D_neg)\n","    \n","    # for each unqiue word\n","    for word in unique_words:\n","        \n","        # get the positive and negative frequency of the word\n","        freq_pos = freqs.get((word,1),0)\n","        freq_neg = freqs.get((word,0),0)\n","        \n","        # calculate the probability that word is positive, and negative\n","        p_w_pos = (freq_pos + 1)/(N_pos + V)\n","        p_w_neg = (freq_neg + 1)/(N_neg + V)\n","        \n","        # calculate the log likelihood of the word\n","        loglikelihood[word] = np.log(p_w_pos/p_w_neg)\n","        \n","    return logprior, loglikelihood\n"],"metadata":{"id":"hHawcX-_ldTK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n","print(logprior)\n","print(len(loglikelihood))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CdVw_dnRliJT","executionInfo":{"status":"ok","timestamp":1650520102244,"user_tz":-330,"elapsed":9,"user":{"displayName":"Rahul P P","userId":"05848758069387796897"}},"outputId":"425b0ed5-31bf-4029-9fa5-6c43c0b2dac8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.0\n","9088\n"]}]},{"cell_type":"markdown","source":["### Predicting Sentimence"],"metadata":{"id":"UwGeFvDglkbL"}},{"cell_type":"code","source":["# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","def naive_bayes_predict(tweet, logprior, loglikelihood):\n","    '''\n","    Input:\n","        tweet: a string\n","        logprior: a number\n","        loglikelihood: a dictionary of words mapping to numbers\n","    Output:\n","        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n","\n","    '''\n","\n","    # TODO: process the tweet to get a list of words\n","    word_l = process_tweet(tweet)\n","\n","    # TODO: initialize probability to zero\n","    p = 0\n","\n","    # TODO: add the logprior\n","    p += logprior\n","\n","    for word in word_l:\n","\n","        # TODO: get log likelihood of each keyword\n","        if word in loglikelihood:\n","          p+= loglikelihood[word]\n","\n","\n","    return p"],"metadata":{"id":"5T7_X41fllxk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for tweet in ['I am not feeling good', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great', 'bad bad bad bad']:\n","    # print( '%s -> %f' % (tweet, naive_bayes_predict(tweet, logprior, loglikelihood)))\n","    p = naive_bayes_predict(tweet, logprior, loglikelihood)\n","#     print(f'{tweet} -> {p:.2f} ({p_category})')\n","    print(f'{tweet} -> {p:.2f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n1dztltRnOG_","executionInfo":{"status":"ok","timestamp":1650520102245,"user_tz":-330,"elapsed":8,"user":{"displayName":"Rahul P P","userId":"05848758069387796897"}},"outputId":"b54c943b-aebe-4064-a297-1037d4a151e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["I am not feeling good -> -0.52\n","I am bad -> -1.29\n","this movie should have been great. -> 2.14\n","great -> 2.14\n","great great -> 4.28\n","great great great -> 6.41\n","great great great great -> 8.55\n","bad bad bad bad -> -5.17\n"]}]}]}